{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.__init__ import *\n",
    "img_folder = \"G:/Github/ocr_hanviet/data/test\"\n",
    "pdf_folder = \"G:/Github/ocr_hanviet/data/pdf\"\n",
    "output_path = \"G:/Github/ocr_hanviet/data/ocr_output.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447f1509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHUY·ªÇN T·ª™ PDF SANG ·∫¢NH JPG\n",
    "for tap_number in range(6, 11): \n",
    "    pdf_path = f\"{pdf_folder}/tap_{tap_number}.PDF\"\n",
    "\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Kh√¥ng th·ªÉ m·ªü file {pdf_path}: {e}\")\n",
    "        continue\n",
    "\n",
    "    for page_number in range(len(doc)):\n",
    "        try:\n",
    "            page = doc.load_page(page_number)\n",
    "            pix = page.get_pixmap(dpi=300)\n",
    "            output_path = f\"{img_folder}/page_{tap_number}_{page_number + 1}.jpg\"\n",
    "            pix.save(output_path)\n",
    "            print(f\"‚úÖ ƒê√£ l∆∞u: {output_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå L·ªói khi x·ª≠ l√Ω trang {page_number + 1} c·ªßa t·∫≠p {tap_number}: {e}\")\n",
    "\n",
    "    doc.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT_OCR:\n",
    "    def __init__(self, api_key: str, img_folder: str):\n",
    "        self.api_key = api_key\n",
    "        self.img_folder = img_folder\n",
    "        self.last_section_id = None\n",
    "        openai.api_key = self.api_key\n",
    "\n",
    "    def _load_image(self, image_path: str) -> Optional[str]:\n",
    "        if not os.path.exists(image_path):\n",
    "            print(f\"‚ùå File kh√¥ng t·ªìn t·∫°i: {image_path}\")\n",
    "            return None\n",
    "        try:\n",
    "            with open(image_path, \"rb\") as f:\n",
    "                return base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Kh√¥ng th·ªÉ ƒë·ªçc ·∫£nh: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _call_gpt_ocr(self, image_data_url: str) -> Optional[str]:\n",
    "        try:\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": (\n",
    "                            \"B·∫°n l√† tr·ª£ l√Ω OCR chuy√™n nghi·ªáp. H√£y tr√≠ch xu·∫•t t·ª´ng D√íNG vƒÉn b·∫£n ti·∫øng Vi·ªát in trong ·∫£nh. \"\n",
    "                            \"V·ªõi m·ªói d√≤ng, tr·∫£ v·ªÅ m·ªôt dictionary c√≥: \"\n",
    "                            \"'text': n·ªôi dung c·ªßa d√≤ng, v√† 'bbox': t·ªça ƒë·ªô bounding box d·∫°ng [x0, y0, x1, y1]. \"\n",
    "                            \"Ch·ªâ tr·∫£ v·ªÅ m·ªôt list c√°c dictionaries, kh√¥ng gi·∫£i th√≠ch th√™m.\"\n",
    "                        )\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [\n",
    "                            {\n",
    "                                \"type\": \"image_url\",\n",
    "                                \"image_url\": {\"url\": image_data_url}\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                ],\n",
    "                temperature=0,\n",
    "                max_tokens=2000,\n",
    "            )\n",
    "            return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error calling GPT-4o API: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _parse_response(self, content: str) -> List[Dict]:\n",
    "        if not content:\n",
    "            return []\n",
    "\n",
    "        content_clean = content.strip()\n",
    "        if content_clean.startswith(\"```\"):\n",
    "            content_clean = re.sub(r\"^```(?:json)?\\s*\", \"\", content_clean)\n",
    "            content_clean = re.sub(r\"\\s*```$\", \"\", content_clean)\n",
    "\n",
    "        try:\n",
    "            ocr_lines = json.loads(content_clean)\n",
    "            if isinstance(ocr_lines, list):\n",
    "                return ocr_lines\n",
    "        except json.JSONDecodeError:\n",
    "            try:\n",
    "                print(\"‚ö†Ô∏è JSON l·ªói, th·ª≠ parse b·∫±ng ast...\")\n",
    "                ocr_lines = ast.literal_eval(content_clean)\n",
    "                if isinstance(ocr_lines, list):\n",
    "                    return ocr_lines\n",
    "            except Exception as e_ast:\n",
    "                print(\"‚ùå Parse th·∫•t b·∫°i:\", e_ast)\n",
    "                print(\"üîç Content preview:\", repr(content_clean[:300]))\n",
    "        return []\n",
    "\n",
    "    def _build_dataframe(self, ocr_lines: List[Dict], image_name: str, paper_name:str, file_id: str) -> pd.DataFrame:\n",
    "        data = []\n",
    "        current_section = None\n",
    "        sentence_buffer = []\n",
    "        bbox_buffer = []\n",
    "\n",
    "        def is_sentence_ending(text: str) -> bool:\n",
    "            text = text.strip()\n",
    "            return (\n",
    "                text.endswith((\".\", \":\", \"!\", \"?\"))\n",
    "                or text.isupper()\n",
    "                or text.startswith(\"-\")\n",
    "                or len(text.split()) <= 3\n",
    "            )\n",
    "\n",
    "        for idx, line in enumerate(ocr_lines):\n",
    "            if not isinstance(line, dict) or \"text\" not in line:\n",
    "                continue\n",
    "\n",
    "            text = line[\"text\"].strip()\n",
    "            if not text:\n",
    "                continue\n",
    "            bbox = line.get(\"bbox\", [0, 0, 0, 0])\n",
    "            if not isinstance(bbox, list) or len(bbox) != 4:\n",
    "                bbox = [0, 0, 0, 0]\n",
    "\n",
    "            match = re.search(r\"(quy·ªÉn\\s+\\w+)\", text, re.IGNORECASE)\n",
    "            if match:\n",
    "                current_section = match.group(1)\n",
    "                self.last_section_id = current_section  # ‚úÖ C·∫≠p nh·∫≠t section ID d√πng cho ·∫£nh ti·∫øp theo\n",
    "\n",
    "            sentence_buffer.append(text)\n",
    "            bbox_buffer.append(bbox)\n",
    "\n",
    "            next_line = ocr_lines[idx + 1][\"text\"].strip() if idx + 1 < len(ocr_lines) else \"\"\n",
    "            ends_now = is_sentence_ending(text)\n",
    "            starts_new = next_line.startswith(\"-\") or next_line.isupper()\n",
    "\n",
    "            if ends_now or starts_new or idx == len(ocr_lines) - 1:\n",
    "                full_text = \" \".join(sentence_buffer).strip()\n",
    "                sentence_buffer.clear()\n",
    "\n",
    "                sentences = [s.strip() for s in full_text.split('.') if s.strip()]\n",
    "                if not sentences:\n",
    "                    sentences = [full_text]\n",
    "\n",
    "                for i, sent in enumerate(sentences):\n",
    "                    full_sentence = sent if sent.endswith(('.', '!', '?', ':')) else sent + '.'\n",
    "                    data.append({\n",
    "                        \"File_id\": file_id,\n",
    "                        \"Paper_id\": paper_name,\n",
    "                        \"Section_id\": current_section or self.last_section_id,\n",
    "                        \"Page_id\": image_name,\n",
    "                        \"BBox\": bbox_buffer[0] if bbox_buffer else [0, 0, 0, 0],\n",
    "                        \"Text\": full_text if i == 0 else \"\",\n",
    "                        \"Sentence\": full_sentence\n",
    "                    })\n",
    "                bbox_buffer.clear()\n",
    "        return pd.DataFrame(data)\n",
    "\n",
    "    def ocr_image(self, image_path: str, image_name: str, paper_name:str, file_id: str = \"HVE_009\") -> pd.DataFrame:\n",
    "        print(f\"üì§ ƒêang OCR: {image_name}\")\n",
    "        image_b64 = self._load_image(image_path)\n",
    "        if not image_b64:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        content = self._call_gpt_ocr(f\"data:image/jpeg;base64,{image_b64}\")\n",
    "        if not content:\n",
    "            print(\"‚ùå Kh√¥ng nh·∫≠n ƒë∆∞·ª£c ph·∫£n h·ªìi t·ª´ GPT\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        ocr_lines = self._parse_response(content)\n",
    "        if not ocr_lines:\n",
    "            print(\"‚ùå OCR lines r·ªóng\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        return self._build_dataframe(ocr_lines, image_name, paper_name, file_id)\n",
    "\n",
    "    def ocr_folder_parallel(self, max_workers: int = 4, output_file: Optional[str] = None) -> pd.DataFrame:\n",
    "        if not os.path.exists(self.img_folder):\n",
    "            print(f\"‚ùå Folder kh√¥ng t·ªìn t·∫°i: {self.img_folder}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        files = sorted(f for f in os.listdir(self.img_folder) if f.lower().startswith(\"page_\") and f.lower().endswith(\".jpg\"))\n",
    "        if not files:\n",
    "            print(\"‚ùå Kh√¥ng t√¨m th·∫•y ·∫£nh h·ª£p l·ªá\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        print(f\"üñºÔ∏è T√¨m th·∫•y {len(files)} ·∫£nh, b·∫Øt ƒë·∫ßu OCR v·ªõi {max_workers} workers...\")\n",
    "\n",
    "        results = []\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            future_to_file = {\n",
    "                executor.submit(self.ocr_image, os.path.join(self.img_folder, f), f.split(\"_\")[-1].replace(\".jpg\", \"\"), f.split(\"_\")[1]): f\n",
    "                for f in files\n",
    "            }\n",
    "\n",
    "            for i, future in enumerate(as_completed(future_to_file), 1):\n",
    "                file = future_to_file[future]\n",
    "                try:\n",
    "                    df = future.result()\n",
    "                    if not df.empty:\n",
    "                        results.append(df)\n",
    "\n",
    "                        # ‚úÖ Ghi append v√†o output_file ngay sau khi c√≥ k·∫øt qu·∫£\n",
    "                        if output_file:\n",
    "                            write_header = not os.path.exists(output_file)\n",
    "                            df.to_csv(output_file, mode='a', index=False, encoding='utf-8-sig', header=write_header)\n",
    "\n",
    "                        print(f\"‚úÖ ({i}/{len(files)}) {file}: {len(df)} d√≤ng\")\n",
    "                    else:\n",
    "                        print(f\"‚ö†Ô∏è ({i}/{len(files)}) {file}: Kh√¥ng c√≥ d√≤ng\")\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå ({i}/{len(files)}) {file}: {e}\")\n",
    "\n",
    "        if results:\n",
    "            df_all = pd.concat(results, ignore_index=True)\n",
    "            print(f\"üìä T·ªïng c·ªông {len(df_all)} d√≤ng t·ª´ {len(results)} ·∫£nh\")\n",
    "            if output_file:\n",
    "                try:\n",
    "                    df_all.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "                    print(f\"üíæ ƒê√£ l∆∞u k·∫øt qu·∫£ v√†o: {output_file}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è Kh√¥ng th·ªÉ l∆∞u file: {e}\")\n",
    "            return df_all\n",
    "        else:\n",
    "            print(\"‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu n√†o ƒë∆∞·ª£c tr√≠ch xu·∫•t\")\n",
    "            return pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñºÔ∏è T√¨m th·∫•y 11 ·∫£nh, b·∫Øt ƒë·∫ßu OCR v·ªõi 10 workers...\n",
      "üì§ ƒêang OCR: 1\n",
      "üì§ ƒêang OCR: 2\n",
      "üì§ ƒêang OCR: 3\n",
      "üì§ ƒêang OCR: 4\n",
      "üì§ ƒêang OCR: 5\n",
      "üì§ ƒêang OCR: 6\n",
      "üì§ ƒêang OCR: 7\n",
      "üì§ ƒêang OCR: 8\n",
      "üì§ ƒêang OCR: 9\n",
      "üì§ ƒêang OCR: 189\n",
      "üì§ ƒêang OCR: 2\n",
      "‚úÖ (1/11) page_6_2.jpg: 8 d√≤ng\n",
      "‚úÖ (2/11) page_6_1.jpg: 6 d√≤ng\n",
      "‚úÖ (3/11) page_7_2.jpg: 8 d√≤ng\n",
      "‚úÖ (4/11) page_6_3.jpg: 17 d√≤ng\n",
      "‚úÖ (5/11) page_6_5.jpg: 18 d√≤ng\n",
      "‚úÖ (6/11) page_6_9.jpg: 18 d√≤ng\n",
      "‚úÖ (7/11) page_6_8.jpg: 18 d√≤ng\n",
      "‚úÖ (8/11) page_7_189.jpg: 15 d√≤ng\n",
      "‚úÖ (9/11) page_6_4.jpg: 16 d√≤ng\n",
      "‚úÖ (10/11) page_6_7.jpg: 17 d√≤ng\n",
      "‚úÖ (11/11) page_6_6.jpg: 12 d√≤ng\n",
      "üìä T·ªïng c·ªông 153 d√≤ng t·ª´ 11 ·∫£nh\n",
      "üíæ ƒê√£ l∆∞u k·∫øt qu·∫£ v√†o: G:/Github/ocr_hanviet/data/ocr_output.csv\n"
     ]
    }
   ],
   "source": [
    "extractor = GPT_OCR(api_key=OPENAI_API_KEY, img_folder=img_folder)\n",
    "df_all = extractor.ocr_folder_parallel(max_workers=10, output_file=output_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
